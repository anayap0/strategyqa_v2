{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Experimentation Overview - QA Modelling and Finetuning\n",
        "\n",
        "To begin with, some key dependencies for our experimentation include:\n",
        "\n",
        "*   jsonlines: handling JSON lines from the datasets\n",
        "*   faiss-cpu, pyserini: for searching for relevant documents\n",
        "*   Transformers library\n",
        "\n",
        "A significant part of this experiment set-up involves preprocessing the data from the StrategyQA and DROP datasets so that it is in the correct format to be processed by the model afterwards. In this data pre-processing stage, we extract and format context, questions, and answers, and then tokenize this information using a T5 tokenizer. The tokenizer prepares the data for the T5 model, which we further fine-tune for our specific needs.\n",
        "\n",
        "We utilized two main models in this step - a model for intermediate QA and a Boolean QA model for retrieving the final answer in a true/false format. A major part of our experimentation included testing different models for the intermediate QA in order to find the model that achieved the highest accuracy with the limited compute resources we had.\n",
        "\n",
        "Model fine-tuning is performed using the prepared datasets. Our main experimentation included testing different values for the number of documents retrieved and k-values, where k is the number of top elements retrieved from the reranked set of documents.\n",
        "\n",
        "We also attempted to fine-tune our intermediate model through prior training on datasets such as the DROP dataset, however, the DROP\n",
        "\n",
        "Finally, the evaluation stage at the end is where we tested our model assessed using the StrategyQA dataset, testing our process based on three metrics - accuracy, SARI, and Recall@10.\n",
        "\n"
      ],
      "metadata": {
        "id": "y9a9AkuBxmaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Set-Up\n",
        "\n",
        "In the following section of code, we complete the base set-up needed for the QA modelling and finetuning later on. This includes importing Google Drive for access to large data files and cloning into our Git repository for accessing the relevant code. We also install the necessary dependencies for searching through the indexed Wikipedia corpus, ensure the code is being run using a GPU, and importing the searcher we need to search through the indexed corpus."
      ],
      "metadata": {
        "id": "amjBN0gtio3K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULapyGFk28F5"
      },
      "outputs": [],
      "source": [
        "# drive and repo setup\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!git clone https://github.com/anayap0/strategyqa_v2.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install dependencies to search index\n",
        "!pip install faiss-cpu\n",
        "!pip install pyserini\n",
        "\n",
        "!pip install jsonlines"
      ],
      "metadata": {
        "id": "YLgrRLRt3I6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Device:', DEVICE)"
      ],
      "metadata": {
        "id": "4eZ6CnYh3CNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Helper methods for document retrieval and question answering"
      ],
      "metadata": {
        "id": "Wzha5xS2newR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get documents for question\n",
        "from pyserini.search import LuceneSearcher\n",
        "searcher = LuceneSearcher('./drive/MyDrive/NLP/sample_collection_jsonl/') ]]\n",
        "import json\n",
        "\n",
        "def getDocsForQuestion(question, searcher, num_docs):\n",
        "  hits = searcher.search(question, num_docs)\n",
        "  formatted_hits = [json.loads(searcher.doc(hit.docid).raw()) for hit in hits]\n",
        "  return formatted_hits\n"
      ],
      "metadata": {
        "id": "v7P2G2cb3aHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from  transformers  import  AutoTokenizer, AutoModelWithLMHead, pipeline\n",
        "\n",
        "general_qa_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-T5-large\")\n",
        "general_qa_model = AutoModelWithLMHead.from_pretrained(\"google/flan-T5-large\").to(DEVICE)\n",
        "\n",
        "def findAnswerToDecomp(question, context):\n",
        "  input = f\"question: {question} context: {context}\"\n",
        "  encoded_input = general_qa_tokenizer([input],\n",
        "                              return_tensors='pt',\n",
        "                              max_length=512,\n",
        "                              truncation=True,\n",
        "                              padding=True).to(DEVICE)\n",
        "  output = general_qa_model.generate(input_ids = encoded_input.input_ids,\n",
        "                              attention_mask = encoded_input.attention_mask, max_length=480)\n",
        "\n",
        "  output = general_qa_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "  return output"
      ],
      "metadata": {
        "id": "MxO-TRp5BhQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "boolean_qa_model = AutoModelForSequenceClassification.from_pretrained(\"shahrukhx01/roberta-base-boolq\").to(DEVICE)\n",
        "boolean_qa_tokenizer = AutoTokenizer.from_pretrained(\"shahrukhx01/roberta-base-boolq\")\n",
        "\n",
        "def findAnswerToLastDecomp(question, context): #TODO Finetune on boolq datasets\n",
        "  sequence = boolean_qa_tokenizer.encode_plus(question, context, return_tensors=\"pt\", truncation=True, padding=True)['input_ids'].to(DEVICE)\n",
        "\n",
        "  logits = boolean_qa_model(sequence)[0]\n",
        "  probabilities = torch.softmax(logits, dim=1).detach().cpu().tolist()[0]\n",
        "  proba_yes = round(probabilities[1], 2)\n",
        "  proba_no = round(probabilities[0], 2)\n",
        "\n",
        "  return \"true\" if proba_yes > proba_no else \"false\"\n",
        "\n",
        "passage = \"\"\"Berlin is the capital and largest city of Germany by both area and population. Its 3.8 million inhabitants make it the European Union's most populous city,\n",
        "                        according to the population within city limits.\"\"\"\n",
        "\n",
        "question = \"Is Berlin the smallest city of Germany?\"\n",
        "print(findAnswerToLastDecomp(question, passage))"
      ],
      "metadata": {
        "id": "oK4EOejiBls0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-large')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-large').to(DEVICE)\n",
        "\n",
        "def getRelevantDocs(question, formatted_hits, k): #reranks the documents and takes the top k elements\n",
        "  # TODO finetune\n",
        "  # create pairs with contents\n",
        "  pairs = []\n",
        "  ids = []\n",
        "  for hit in formatted_hits:\n",
        "    ids.append(hit['m_id'])\n",
        "    pairs.append([question, hit['contents']])\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  # find outputs\n",
        "  with torch.no_grad():\n",
        "    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512).to(DEVICE)\n",
        "    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n",
        "\n",
        "  # sort by score\n",
        "  pairs_with_scores = list(zip(pairs, scores, ids))\n",
        "  sorted_pairs = sorted(pairs_with_scores, key=lambda x: x[1], reverse=True)\n",
        "  sorted_pairs_only = [(pair, id) for pair, score, id in sorted_pairs]\n",
        "  return sorted_pairs_only[0:k]"
      ],
      "metadata": {
        "id": "1p5lN9YwBl4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this function takes in a single json line in t5predictions.jsonl as input and find the final answer\n",
        "def processQuestion(decompositions, doc_num, k_val):\n",
        "  answers = []\n",
        "\n",
        "  documents = set()\n",
        "  # for each decomposition, find context and question\n",
        "    # this also involves substituting in # markers with relevant previous answers\n",
        "    # need to figure out how to handle out of bounds # markers\n",
        "    # assume every question needs context\n",
        "  # then, use model that finds answer on this question and context\n",
        "  # then, append answer to answers\n",
        "\n",
        "  for i in range(len(decompositions)):\n",
        "    replaced_decomp = decompositions[i]\n",
        "    new_decomp = decompositions[i]\n",
        "    # need to figure out how to replace references\n",
        "    if len(answers) != 0:\n",
        "      while '#' in new_decomp:\n",
        "        num = int(new_decomp[new_decomp.index(\"#\") + 1])\n",
        "        if num >= 0 and num < len(answers):\n",
        "          replaced_decomp = replaced_decomp.replace(f\"#{num}\", answers[num - 1])\n",
        "        else:\n",
        "          replaced_decomp = replaced_decomp.replace(f\"#{num}\", answers[-1])\n",
        "        new_decomp = new_decomp[new_decomp.index(\"#\") + 2:]\n",
        "\n",
        "    hits = getDocsForQuestion(replaced_decomp, searcher, doc_num) # TODO experiment with the 20, 3 for best results\n",
        "    relevant = getRelevantDocs(replaced_decomp, hits, k_val)\n",
        "    context = \" \".join([info[1] for (info, id) in relevant])\n",
        "\n",
        "    for (info, id) in relevant:\n",
        "      documents.add(id)\n",
        "\n",
        "    answers.append(findAnswerToDecomp(replaced_decomp, context) if i < len(decompositions) - 1 else findAnswerToLastDecomp(replaced_decomp, context))\n",
        "    # print(f\"epoch {i}:\")\n",
        "    # print(f\"    {answers}\")\n",
        "\n",
        "  return (answers, list(documents))"
      ],
      "metadata": {
        "id": "MuKjBA4kBdWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('/content/strategyqa_v2/data/optimal_break_decomps.json') as file:\n",
        "  decomposition_data = json.load(file)\n",
        "\n",
        "print(decomposition_data)"
      ],
      "metadata": {
        "id": "2SlGspM33cpb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c4a2bb1-5c93-4160-e8bf-79cc00810277"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'e0044a7b4d146d611e73': {'decomposition': ['How many occupants can the Albany in Georgia reach?', 'What is the population of New York?', 'Is #1 greater than #2?']}, 'c69397b4341b65ed080f': {'decomposition': ['What language is Saint Vincent and the Grenadines from?', 'Where is #1 found?', 'Is #2 in English?']}, 'be5c9933987f046b476e': {'decomposition': ['What are the Seven Deadly Sins?', 'What is greed?', 'Is #1 the same as #2?']}, '1932e05f10680ece229f': {'decomposition': ['How high is the top of Mount Fuji?', 'Where is the Sea of Japan located?', 'Is #1 higher than #2?']}, 'fb8b656051c742f5bd27': {'decomposition': ['What is the highest ranked song in Billboard?', 'Who were the members of The Lox?', 'Is #2 included in #1?']}, 'c91eafafed5a8f80bb5a': {'decomposition': ['Which areas are known as the American West Coast?', 'Is Miami included in any of #1?']}, '2047c0c34383f8014820': {'decomposition': ['How many people are in the Virginia General Assembly?', 'How many people are in the Swiss Guard?', 'Is #2 greater than or equal to #1?']}, '7702ee1e9f757ebffdf1': {'decomposition': ['Which countries participated in the Portuguese Colonial War?', 'Which countries did any of #1 belong to?', 'Which countries were part of #2 in #2?', 'Is any of #3 included in #2?']}, '11d009721f27a60f9cff': {'decomposition': ['What was the name of the ancient Englishman who was once located in Pict?', 'What was the name of the ancient Englishman who died in #2?', 'Is #1 the same as Old English?']}, '2a90188d5b82d12c036d': {'decomposition': [\"What was Lil Wayne's first driving age?\", 'What is the minimum age required to use a vehicle?', 'Is #1 greater than or equal to #2?']}, '1e9d59987a695898808f': {'decomposition': ['What is the crucifix found in Karachi?', 'Where is the crucifix found?', 'Which places are crucifixes found in?', 'Is there any overlap between #1 and #2?']}, '59e78fdde03974a7cf77': {'decomposition': ['When was the Birth of Venus published?', 'What year was the Creative Commons License issued?', 'Is #2 before #1?']}, '869bbd1c4e3c0bf02527': {'decomposition': ['What is the weight of a six year old?', 'How much can ten gallons of seawater crush?', 'Is #1 greater than #2?']}, '4a28d99dcfc14161dc9f': {'decomposition': ['What temperature range do anchovy live in?', 'What is the temperature range of water that are warmer than #1?', 'What is the temperature range of water that are warmer than #2?', 'Is #3 lower than #4?']}, '9635db8809b449470dd6': {'decomposition': ['What do 3D printing machines do?', 'What properties do #1 have?', 'Does adenovirus have #2?']}, '44f54e361e0f46ead5d4': {'decomposition': ['George Fox does not support stoning?', 'Does George Fox support #1?']}, 'addf92ab71aca4e783b1': {'decomposition': [\"What is Darth Vader's real name?\", 'What is the name of the person who is #1?', 'Which army does Darth Vader join?', 'Is #2 included in #3?']}, 'f9686fe476e2d06e4dab': {'decomposition': ['What type of programming do computers run on?', 'What are the basic features of Boolean algebra?', 'Does any of #1 use #2?']}, '5737e93c88fa5e21c2b5': {'decomposition': ['Who was the host of The Voice show?', 'How old was Christina Aguilera when she died?', 'Is #2 before #1?']}, '33421d4495e27f91be4d': {'decomposition': [\"What was Amy Winehouse's death?\", 'Can Narcan prevent #1?']}, '99eb858b4c9624a71b40': {'decomposition': ['What does the mongoose stand for?', 'What is the main ingredient in #1?', 'What is the main ingredient in #2?', 'Is #3 included in #3?']}, 'b1e1256007b0a4a341a7': {'decomposition': ['What are the ingredients in a buffalo wings?', 'Is capsaicin a highly active ingredient in #1?']}, '12b0bb830de101c0f118': {'decomposition': ['When did the 1964 University of Washington graduation?', 'When was Bruce Lee born?', 'Is #2 before #1?']}, '0f28837d8ce6901345bf': {'decomposition': ['What is the minimum age to succeed at University of Pennsylvania?', 'What is the minimum age required to meet the minimum requirement for admittance to the University of Pennsylvania?', 'Is #2 greater than or equal to #1?']}, '3545982eb15f96652e1b': {'decomposition': ['What is the minimum age required to join the rowing team?', 'What is the minimum age required to meet #1?', 'Is #2 larger than or equal to #3?']}, 'a19804759885b694c56a': {'decomposition': ['How many lolcats are on a first generation iPhone?', 'What is the size of #1?', 'Is #2 greater than 100?']}, '51678920a34b51e1e355': {'decomposition': ['What does Rusev claim to be?', 'What is the population of #1?', 'Is #2 greater than the population of Rusev?']}, 'ebd53fd053c84d26f889': {'decomposition': ['What is the typical shape and color of a peach fruit?', 'What is the pattern and color of Princess Peach?', 'Is #1 similar to #2?']}, 'cbebe1a0113581f37141': {'decomposition': ['What is the minimum age for patients to be admitted to the United States Air Force?', 'What is the minimum age for patients to be admitted to the psychiatric setting?', 'Is #2 greater than #1?']}, 'badd24672a2f951706fe': {'decomposition': ['What is the average size and weight capacity of a basket?', 'What is the size and weight of a Bing?', 'Is #2 smaller than #1?']}, 'e5ffcc7b22a58df8952d': {'decomposition': ['What is the main processing unit of the CPU CPU Unit?', 'How many fans are needed for a specific CPU unit?', 'Is #1 divided by #2 greater than or equal to one?']}, '58f7df5e3836b24b08d0': {'decomposition': [\"What was Cyril Ramaphosa's position on NATO?\", 'How is #1 related to?', 'Which country is Cyril Ramaphosa a head of?', 'Is #2 the same as #3?']}, 'e144fef6c590823af46a': {'decomposition': ['What food is Brooklyn known for?', 'Is #1 known for its bread products?']}, '8d95e7d922024a684ac0': {'decomposition': ['When did the Royal Air Force fight in the Boxer Rebellion take place?', 'When was the Boxer Rebellion formed?', 'Is #2 before #1?']}, '427fdafa9e7047587d75': {'decomposition': [\"When did François Mitterand's service in France begin?\", 'When was Napoleon Bonapart stationed?', 'Is #2 before #1?']}, '2025983d427a9d3d5bab': {'decomposition': ['What is cancer spread?', 'What is the cause of #1?', 'Does #2 include someone with cancer?']}, '0edac4af92465027fe27': {'decomposition': ['Which organ in the body does Cholera live in?', 'Is #1 the same as Cholera?']}, '520becb10b5c138ab300': {'decomposition': ['What ingredients are on a sweet potato?', 'What are the ingredients in a pineapple?', 'Is any of #1 included in #2?']}, '8757a53800192e066503': {'decomposition': ['Which animal is a penguin?', 'What is the climate of Miami?', 'Is there an overlap between #1 and #2?']}, '4330d46b6f594ad122a6': {'decomposition': ['Which organ of the body are affected by a tumor?', 'Is #1 hidden from view?']}, 'a1a131e5a540d8eff3c4': {'decomposition': ['When did the MCU start fighting against Falcons?', 'When was Spiderman born?', 'Is #2 before #1?']}, 'f85416430e1ab2d6f96b': {'decomposition': ['What were the physical characteristics of the Shiva dance?', 'What were the physical characteristics of the ancient physical fitness pose?', 'Is any of #1 included in #2?']}, '042463e9ca7cd4c7eb5a': {'decomposition': ['Which animals are featured in a Broadway musical?', 'Are Hyenas included in #1?']}, '9fa4b205aaca877a15bf': {'decomposition': ['What kind of animal are owl monkeys?', 'What color is an owl?', 'Are strawberries harmless to #1?']}, '34198157c2dd7028f0a4': {'decomposition': ['What is the weight range of a snake swallow?', 'What is the weight range of an M60 Patton?', 'Is #1 larger than #2?']}, '9c13ff296c299f0cd02d': {'decomposition': ['Which comic character did the Avengers take on?', 'What comic store did DC Comics sell?', 'Is #1 not in #2?']}, 'ed04a34363f248900c18': {'decomposition': ['What is the area of the Persian Gulf?', 'What is the area of New Jersey?', 'Is #1 less than #2?']}, 'cda79ce499af9dae8ffb': {'decomposition': [\"What is Nicole Kidman's favorite food?\", 'What is the average weight of #1?', 'What is the average weight of a Psylocke based on?', 'Is #2 smaller than #3?']}, '2ef10335771662b59cf8': {'decomposition': ['How much is the Metropolitan Museum of Art twenty times?', 'How much is Bernie Sanders worth?', 'Is #2 less than two hundred dollars?']}, '983e2713b89f38cab35a': {'decomposition': ['What is the name of the organization that Planned Parenthood works for?', 'What does #1 stand for?', 'What does the Herpes simplex virus stand for?', 'Are #2 and #3 the same?']}, '018cb17cb34d19be99a6': {'decomposition': [\"What is the main topic of the Achilles' kingdom?\", 'How many people are in the Legolas kingdom?', 'How many people are in the #1 of #2?', 'Is #3 greater than #4?']}, '3a76a62be2518008c701': {'decomposition': ['What country is Bucharest located in?', 'Is #1 located south of Egypt?']}, 'e1f9a6b9fbba8d014643': {'decomposition': ['What does a rock float in?', 'What is the density of the atmosphere of Earth?', 'Is #1 less than #2?']}, '49ab746264f2e50e9aee': {'decomposition': ['Which disease is referred to as sex deaths?', 'What causes rhinos to become life-threatening?', 'Is #1 improved by the use of animals?']}, 'a9ac24cbbb2fce862026': {'decomposition': ['Who was the MCBA founder of?', 'Who were the parents of The Mamas & The Papas?', 'Is #1 the same as #2?']}, 'e537c420c9cb2e24409c': {'decomposition': ['Who was the author of Little Women?', 'When did #1 die?', 'When was the 13th Amendment passed?', 'Is #2 before #3?']}, 'c42d951f2b8339998c71': {'decomposition': ['What is the Pledge of Allegiance?', 'What is the popular topic in Reddit?', 'Which group of people are familiar with #2?', 'Is #2 the same as most of #1?']}, 'c4f5cedc6af621e44eb5': {'decomposition': ['What is the address of children when they send Christmas letters?', 'Is the South Pole included in #1?']}, '6a0d8d74c097292a5d38': {'decomposition': ['When did the Maroon 5 tour?', 'When was Nirvana born?', 'Is #2 before #1?']}, '79535eed1af03ca748c5': {'decomposition': ['How old was the Evander Holyfield boxing match?', 'What is the age of the oldest boxing coach ever recorded?', 'Is #2 greater than #1?']}, 'f2a71092d2cfe16d9a64': {'decomposition': ['What does Post Malone hate?', 'Can needles be a stinger when they are in #1?']}, '088eb1d9e2f067cd4f36': {'decomposition': ['When was the American president in 1936?', 'When did Abraham Lincoln die?', 'Is #1 before #2?']}, '2fcd999dfde755c5c049': {'decomposition': ['Which romantic partner was Eve involved in in the incestuous relationship?', 'Is #1 incestuous relationship?']}, '9a6e08a0978b9dbc8f1d': {'decomposition': ['What is the real name of the rapper Lil Wayne?', 'How many Grammy Awards has #1 won?', 'How many Grammy Awards has a Grammy Award won?', 'Is #2 greater than #3?']}, '2e4a3ac18a5292ee1735': {'decomposition': ['What is the outer core of the earth?', \"What is nickel's temperature?\", 'Is #2 higher than the surface of the earth?']}, '4b6ca118a638ad3539c8': {'decomposition': ['Where was Dorothea Wendling born?', 'Where was Porsche invented?', 'Is #1 the same as #2?']}, 'd0e463f95b1221a55a13': {'decomposition': ['Which emergency is caused by a lack of what?', 'What is the cause of an existential crisis?', 'Does surgery preclude #1?']}, 'c435c8acf644ef0dde15': {'decomposition': ['When did Al-Farabi die?', 'When was the Great Sheikh founded?', 'Is #2 before #1?']}, 'd2ae10dfd3fa8c338d2d': {'decomposition': ['What is the chemical reaction of chlorine?', 'What happens when you mix sodium and #1 together?', 'Is #2 dangerous?']}, 'b73a1bc6f2f2eb0058e8': {'decomposition': ['Who founded the jiu-jitsu Gracie?', 'How many children were in the group of #1?', 'How many children are in the group of #2?', 'Is #3 at least equal to or greater than #4?']}, '90c5595a6ba03e90b2c3': {'decomposition': ['When did Mozart die?', 'When was Dolce & Gabbana founded?', 'Is #2 before #1?']}, '8f9ba93cb04e4f652733': {'decomposition': [\"What type of clothing did the 1700's celebrate?\", 'What type of clothing is an Orthodox Presbyterian?', 'Would #2 be considered #1?']}, 'b848db708048f54dfb6c': {'decomposition': ['How long are the lines on the Antiquity of Microbes?', 'How long is the average haiku?', 'Is #1 shorter than #2?']}, 'a651ba82c5e39990d737': {'decomposition': ['What directors are the directors of The Matrix?', 'What are the main beliefs of people who have been #1?', 'What are the main beliefs of women who have been #2?', 'Is transgender rights included in #3?']}, '117c27244c8e00112265': {'decomposition': ['What is the minimum driving test required of a northern fur seal?', 'What is the minimum driving test required of a driver to pass the #1?', 'Is #2 greater than the range of #3?']}, '83a7c67c6f156f9cf4cb': {'decomposition': ['How many people visit Taco Bell each year?', 'How many people visit the Roy Rogers every year?', 'Is #1 greater than #2?']}, 'abb3562932923dc9286e': {'decomposition': ['What kind of water habitat is the Red Sea located in?', 'Do yellow perch live in #1?']}, '67349fa25d548ca128ec': {'decomposition': [\"What is General Zod's favorite food?\", 'What type of food is a Samsung Galaxy S4?', \"What is #2's favorite food?\", 'Is #3 more similar to #1 than #2?']}, 'e126c8162ff2b480e898': {'decomposition': [\"What was Neanderthals' profession?\", 'What subjects did #1 use in their book lineup?', 'Is #2 arithmetic?']}, '8238ca7e53b2eadfa5fe': {'decomposition': ['Which animal was Charlie Brown, the hound?', 'Is #1 the same as #2?']}, '34aaa0304d840b427cc3': {'decomposition': ['What genre is the \"And Then Were None\" written in?', \"Which genre is #1's books in?\", \"Is #2 the same as JK Rowling's works?\"]}, 'c649c9f1814bd72b8ccd': {'decomposition': ['What artists did Jay-Z date in 1999?', 'Who was the artist Louis Armstrong date in 1999?', 'Is #2 before #1?']}, 'c7cb1e3fc112e8f924fe': {'decomposition': ['What are the lengths of the longest in the UK?', 'Is length in metres one of #1?']}, '8d7f7bca01ee50ec0dc7': {'decomposition': [\"What was Christopher Columbus' original home country?\", 'What country was #1 in?', 'What country was Christopher Columbus born in?', 'Is #2 different from #3?']}, 'ee89128e112aca3b7efd': {'decomposition': ['What are the shades of blue lips?', 'Is any of #1 the same as blue lips?']}, '0de2785a6eaba087541a': {'decomposition': ['What does Jon Brower Minnoch suffer from?', 'What is the cause of anorexia nervosa?', 'Is #2 the same as #1?']}, '4bea34763a1e9f0d5b2b': {'decomposition': [\"Which of Shakespeare's plays was most famously known during the Renaissance?\", 'When did Scheherazade die?', 'Was #1 the King?']}, '51126206a7baab47253d': {'decomposition': [\"When was Hillary Clinton's deputy chief of staff in 2009?\", 'When was #1 baptism performed?', 'Is #2 before #3?']}, '89163b1a436926698578': {'decomposition': [\"What is the Cassowary's favorite food?\", \"What is the weight of a crane's favorite food?\", 'Is #1 greater than #2?']}, 'ab2527bb6fd97508b566': {'decomposition': ['What is the distance between New England and Sainsbury?', 'What is the distance between #1 and #2?', 'Is #2 more than #3?']}, '7c287c8c9a4b3aef8e75': {'decomposition': [\"What is P. G. Wodehouse's second grade grade?\", 'What is the age range for teachers who teach #1?', 'Is #2 the same as P. G. Wodehouse?']}, 'bbfc68abfe40adb1c618': {'decomposition': ['What are the characteristics of tobacco?', \"Are the characteristics of Alice's Adventures in Wonderland among #1?\"]}, 'cbc0979b3b041cc01f32': {'decomposition': [\"What is Simon Cowell's citizenship requirement to be a citizen of the United States?\", 'When will the next Supreme Court judge be appointed?', 'Is #2 before #1?']}, '316d74f3e8ce42728fdd': {'decomposition': ['How many people were unemployed?', 'How many people were in the Tiger Stadium?', 'Is #2 greater than #1?']}, '988f73f260f65621866e': {'decomposition': ['How tall was Donatello?', 'How tall is the Sistine Chapel ceiling?', 'Is #1 greater than #2?']}, '29574176246ede705240': {'decomposition': ['Which countries were involved in the Arab-Israeli conflict?', 'Which countries were involved in the Arab-Israeli conflict?', 'Is England included in any of #1?']}, 'd7ea60bc4dd6e8986d4e': {'decomposition': ['What were the name of the children of Alice in Wonderland?', 'What was the name of the young man who died most recently?', 'Is #1 the same as #2?']}, '6d92b708dfcdd1b904be': {'decomposition': ['What colors do Asian black bears vary in color?', 'Is #1 multicolored?']}, '3a9069abb5a9da1e96a3': {'decomposition': ['What rank in a pirate lieutenant?', 'What rank in a navy lieutenant?', 'What rank in a #1?', 'What rank in a navy lieutenant?', 'Is #2 the same as #3?']}, '77b1da70e82d4f165c39': {'decomposition': ['What is the lorem ipsum paragraphs?', 'What is the length of a website containing #1?', 'Is #2 greater than or equal to #3?']}, '21e352756ce2d1add415': {'decomposition': ['What is the land area of beaver dams?', 'What is the land area of beaver dams?', 'Is #1 close to #2?']}, '66b3cfaa499773bdf513': {'decomposition': [\"What was Will Ferrell's first appearance in the Empire Award for Best Newcomer?\", 'Was Will Ferrell ever win #1?']}, '5378f4b41639a2676635': {'decomposition': ['When did Cynthia Powell die?', 'When was John Lennon born?', 'Is #2 before #1?']}, '2ebff3186955f326f556': {'decomposition': ['When did Al Pacino die?', 'When did World War II end?', 'Is #2 before #1?']}, 'ba83e0960e20b69be901': {'decomposition': ['What genre is the Gujarati script in?', 'What genre is the Kanji script in?', 'Is #1 the same as #2?']}, '563a36aa0389c6f96cc7': {'decomposition': ['Who was the city of Morris County named after?', \"Who was #1's father?\", 'Did #2 have a chief justice?']}, '4111c4147ebc06bd1163': {'decomposition': ['What is the size of the largest mountain on Earth?', 'What is the size of the Very Large Telescope?', 'Is #1 larger than #2?']}, '7e69e7d67ecf090a45be': {'decomposition': ['What is the main ingredient in cow methane?', 'What is the main ingredient in cars?', 'Is #1 greater than #2?']}, '9eba4476b61fc6a2dcdc': {'decomposition': ['What conditions are necessary for water skiing?', 'Which country is Morocco located in?', 'Is #2 included in #1?']}, 'f647907820a7ae1d4300': {'decomposition': ['How much does one Amazon share?', 'How many months are in a twenty year Netflix subscription?', 'Is #1 greater than #2?']}, '1b6cc24a9abe52c6ff88': {'decomposition': ['Where did Nikola Tesla once work?', 'Is there radiation in #1?']}, '486598e9d21bb65ea56c': {'decomposition': ['When was the Harlem Renaissance?', 'When was Al Capone born?', 'Is #2 before #1?']}, 'c5e130e153c93e692833': {'decomposition': ['What is the maximum number of people that can participate in the Blue Lives Matter movement?', 'What is the minimum number of people that can participate in #1?', 'Is #2 greater than or equal to #3?']}, '37673881c0036bffce3b': {'decomposition': [\"Which date does New Year's Day occur on each year?\", 'Does #1 always fall on a Wednesday?']}, '44ec5bab0ba6b687d60e': {'decomposition': ['What is the density of a nymph tick?', 'What is the density of a standard hole punch?', 'Is #1 greater than #2?']}, '2c391bc83138f934f965': {'decomposition': ['What is the average price of an art dealer?', 'What was the Da Vinci painting?', 'Is something priced as #2 typically higher than #1?']}, 'c0c6685bdfb09af180a1': {'decomposition': [\"What is Peter Griffin's profession?\", 'In what craft is brewing?', 'Is #1 the same as #2?']}, '14a1db88f1abd55a1286': {'decomposition': ['What were the the 2008 Summer Olympics?', 'How many people can the Rowe 550 seat?', 'Is #2 greater than #1?']}, '3f4f06f08f8c926c68df': {'decomposition': ['When did Amy Winehouse die?', 'When was Star Wars Rogue One released?', 'Is #2 before #1?']}, '54fbd5a53d29864cbb90': {'decomposition': ['What is the Snoopy-themed award given to?', 'Who was the winner of the #1?', 'What is #2 for?', 'Is #3 the same as #2?']}, '23506d586f694c3fc8ec': {'decomposition': ['What is the size of the honey badger?', 'How big is the size of an oven?', 'Is #1 larger than #2?']}, '486b0e16b6c36314378c': {'decomposition': ['What is Chaff?', 'What powers #1?', 'Does #2 have hydropower?']}, '0ad62a523a3634a2a12a': {'decomposition': [\"What is Paulo Coelho's profession?\", 'How old is #1?', 'Can women be trained in #2?']}, '2a7d147c51c82ad1e13d': {'decomposition': ['What is the water habitat of the Hooke Sea?', 'In what body part are lifeboats found?', 'Is #1 located in #2?']}, 'bd9ccc3da0f6467dc411': {'decomposition': ['Which model of Fiat Chrysler was made in the year 1980?', 'Was #1 popular among the Japanese?']}, '6396fef048f5231560ed': {'decomposition': ['What is the real estate of the Hades?', 'What is the real estate of the Osiris?', 'Is #1 comparable to #2?']}, 'a5f8af1dd0e9c46c47be': {'decomposition': ['What is the minimum area that a plane can be slowed in Orange County, California?', 'Is #1 less than or equal to #2?']}, 'f790b706fb7406d30160': {'decomposition': ['When were Princes fully operational?', 'When did June 2020 end?', 'Is #2 before #1?']}, 'bcd221573f1a6500f2eb': {'decomposition': ['What skills are required to be a retail job?', 'What skills are required of someone who has #1?', 'Is #2 also in #3?']}, 'e18f8956e3fd4e8c50b2': {'decomposition': ['What was the material used in a gunpowder storage facility?', 'What was the density of a supersonic shock wave?', 'Is #1 the same as #2?']}, '4f5f3dc321468c1d052d': {'decomposition': ['What are the things you can buy in dollar stores?', 'Is chlorine chlorine chlorine found in #1?']}, 'b4e6585cdc2765c29070': {'decomposition': ['What are the species of saltwater crocodiles?', 'Which species are the families of alligators?', 'Are any of #1 found in the same location as #2?']}, '3679391b4ff09377acef': {'decomposition': ['What would a landscape architect need to know in order to carry out #1?', 'What would Persephone be a good consultant to have in the design of landscapes?', 'Would a landscape architect be able to do #2?']}, '5d85be9c8f41a21ca293': {'decomposition': ['What crew were on the Apollo 15 mission?', 'Does #1 not require a unicycle?']}, '4d919e6c4316cb2e1f09': {'decomposition': ['What tools are required to operate a frigate ship?', 'Do ropes require ropes?']}, '70642ac7ad94f22b612f': {'decomposition': ['What are the major things Dr. Disrespects have in common with?', 'What are the common items in Jason?', 'Is any of #2 included in #1?']}, 'f93ca5fd96248f765ff3': {'decomposition': ['What do people that have a Celiac suffer from?', 'What foods are forbidden in #1?', 'Are spaghetti included in #2?']}, '302570f3787646a587df': {'decomposition': ['What is the main diet of Koalas?', 'What is the main diet of meat?', 'Is #1 more similar to #2 than #2?']}, '48cf1d62fdc4ae46d79e': {'decomposition': ['What nutrient is critical for bodybuilding?', 'How much elk burger contains?', 'How much burger is on a beef burger?', 'Is #2 greater than #1?']}, '596b615b7261cb7db9d3': {'decomposition': ['What is the sea of mercury?', 'What is the container length of a coin?', 'Is #2 greater than #1?']}, '42608d4f0243d307f6de': {'decomposition': ['What happens to an olive tree when it is cut?', 'What happens to an olive tree when it is cut?', 'What happens to #2 when #1 occurs?', 'Is #3 happened in the real world?']}, '0ff6800fcdbfabb483b8': {'decomposition': ['Which nutrients are important for people with adrenaline fatigue?', 'Are Brussels sprouts low in #1?']}, 'e1be1275b3c816b0b7bf': {'decomposition': ['Where is Family Guy located?', 'Is the American West Coast located in #1?']}, 'f7fa93e91293615bc50d': {'decomposition': ['What is the minimum age for someone to work for Mitsubishi?', 'What is Uberlandia?', 'Is #2 larger than #1?']}, 'e33a81482ffef9456bcc': {'decomposition': [\"What was Richard Nixon's political party affiliation?\", 'Does the Watergate scandal affect #1?']}, 'd0e81c46892d0983f1e8': {'decomposition': ['When was Eddie Murphy born?', 'What is the age to dial 911?', 'Is #1 before #2?']}, '1bdcf91a9d90ba61339a': {'decomposition': ['What was the weather condition at Mount Wycheproof in the UK?', 'What was the weather condition in Edmund Hillary?', 'Is #1 the same as #2?']}, '96844c5fba3f4ebacf0d': {'decomposition': ['What are the colors of the rainbow?', 'What are the colors of the Gabon flag?', 'Is any of #1 included in #2?']}, '1cfd64486a9cabeb7c0c': {'decomposition': ['What is the density range of mollymawks?', 'What is the density range of albatrosses?', 'Is #1 greater than #2?']}, 'fc0bbbfd4467bd868714': {'decomposition': ['What equipment is required for curling?', 'What tools are required for curling?', 'Is any of #1 part of #2?']}, '615aa49cbd52bbe0654b': {'decomposition': ['What are emus endemic to?', 'Which elks were a part of in the elks family?', 'Is any of #2 a part of #1?']}, '0f172df66b358ed72ca6': {'decomposition': ['What kind of cuisine is La Grenouille known to serve?', 'Which country is #1 from?', 'Is #2 a country or state?']}, '0abc04530e898056a6c4': {'decomposition': ['What is the main purpose of the Globalization of the United States?', 'What concerns are there about #2 concerning the system of a Down?', 'Is #1 or #2 not related to #3?']}, 'fda4d970e12b123e59e6': {'decomposition': ['What is the average temperature in New England?', 'What is the prevailing temperature in coffee?', 'Is #1 greater than #2?']}, 'ff21df087958bda81be8': {'decomposition': ['What numbers are used in tennis?', 'Does #1 include prime numbers?']}, 'e5dff908bcbcccd5b9a6': {'decomposition': ['How much does 5 years Capital One Venture pay?', 'How much does the Church of Satan cost?', 'Is #1 greater than #2?']}, 'f2eac6cbbde0c16b8ff1': {'decomposition': [\"What genre of music did Beethoven play in the '60s?\", 'What genre of music does EDM play?', 'Is #1 the same as #2?']}, 'd60720f31550d49b03d8': {'decomposition': ['What kind of music does Metallica play?', 'What kinds of music are played by Metallica?', 'Are any of the tracks in #1 also found in #2?']}, '6252e913d7d9401e582f': {'decomposition': [\"How many people can attend Kelly Clarkson's show?\", 'How many people are in the American Idol?', 'Is #2 greater than #1?']}, '4b66ebd989be42113847': {'decomposition': ['What is the black-tailed jackrabbit?', \"What is the European wildcat's fear of?\", 'Is #1 the same as #2?']}, 'c23b892b2ce50d0582d1': {'decomposition': [\"Where is Carl Friedrich Gauss's residence?\", 'Where is 100 miles away from #1?', 'Is #2 100 miles away from #3?']}, 'd24a5b996aaa758bbc80': {'decomposition': ['Which Lionel Richie is the sister of Sheila E?', 'Which animals were the famously known during #1?', 'Is Sheila E a part of #2?']}, 'e0c10e16a23989f7a583': {'decomposition': ['Which number is the FDA required to sell?', 'When were Roman Numerals introduced?', 'Is #2 before #1?']}, '7f457f00fa010a9dd2e8': {'decomposition': ['What is the name of the shanties that can be found on the Oregon Trail?', 'When did #1 occur?', 'Is #2 before the end of the Oregon Trail?']}, 'c0e0a030b4bc54ef6d3b': {'decomposition': ['What is the most common cause of leukophobia?', 'Is the Flag of the United States included in #1?']}, 'fb3862c0ef8ef7e95bce': {'decomposition': ['How many episodes were there on the Desperate Housewives?', \"How many episodes was Teri Hatcher's longest?\", 'Is #2 twice as many episodes are there on the Desperate Housewives?']}, 'f37c8f9e4fc20479b06c': {'decomposition': ['How many blood cells are found in the human body?', 'How many red blood cells are found in the human body?', 'Is #2 greater than #1?']}, '798529ef6d7ff2162dad': {'decomposition': ['Where was John George Bice born?', 'Where was the place where #1 is known to be located?', 'Is #2 near Cornwall?']}, 'c8eac5cc49c449e28e6e': {'decomposition': ['When did the dinosaurs begin?', 'When was the dinosaur age discovered?', 'Is #2 before #1?']}, '55ac71fc1cd8fdc34e8c': {'decomposition': [\"What was Paul Bunyan's job description?\", 'What would he do to plan ahead?', 'Would a poor planner be able to afford #2?']}, 'a68facab7157ed7e520e': {'decomposition': ['Which element are the elements needed for photosynthesis?', 'Is #1 the same as the atmospheric oxygen of Mars?']}, 'ec06b5fcf3cdef642f6e': {'decomposition': ['When did Bill Nye die?', 'When was Franklin Delano Roosevelt born?', 'Is #2 before #1?']}, '09734b1d720aa09e8ef2': {'decomposition': ['What does the word Gypsy involve?', 'What are Romani people known to have said about people in #1?', 'Is #2 included in #3?']}, '7e7ba5f2d4c84b9e2259': {'decomposition': ['Which place was the Land of Israel in 16th century?', 'Who was the capital of an Islamic empire in 16th century?', 'Did #1 reign before #2?']}, 'cc56bc3a2a756fcc8dec': {'decomposition': ['What is the maximum sodium a day recommendation from the USDA?', 'How much sodium is in a zucchini squash?', 'Is #2 greater than the maximum value of #1?']}, '8803ea63343c400758cf': {'decomposition': ['When was the first car manufactured?', 'When was the Mini, the first car manufactured?', 'Is #2 before #1?']}, 'e8d245781a7fd9e38701': {'decomposition': ['What are dehydrators used to make Paprika?', 'Which items are required for each of #1?', 'Is Paprika absent from #2?']}, '19b40f33fa199c1673fd': {'decomposition': ['What is the most food a Bengal cat can eat?', 'Is pancakes limited to #1?']}, '6fe610c55315019e87e5': {'decomposition': ['What is the distance that a wandering albatross can fly?', 'What is the distance between #1 and New York City?', 'Is #2 less than #3?']}, 'dca3c4acc079bb11689b': {'decomposition': ['Which company is Alpo?', 'What does #1 produce?', 'Is the New York Public Library a kind of any of #2?']}, '9ebaee3511157e78ec7b': {'decomposition': [\"What is Tony Bennett's middle name?\", 'Who is the former UFC champion?', 'Is #2 the same as #1?']}, '53f2558c6a9d6c3b0a81': {'decomposition': ['What gender is a peach woman?', 'Does #1 have black widows?']}, '2b73dae1804aae94e1ba': {'decomposition': ['What is the typical temperature range for sunburn?', 'What is the typical temperature range for sunburn?', 'Is #1 greater than #2?']}, '243e3e1ef3fb7190ace3': {'decomposition': [\"What is Rahul Dravid's profession?\", 'In what field or industry is #1?', 'What is the field goal of #2?', 'Is #3 the same as #2?']}, '62e8d2b87e80f78b152c': {'decomposition': ['What do oceanographers study?', 'Which subject is regarded as an oceanographer?', 'Is #1 a subject of study of #2?']}, '3a93c4d5d6dca707a016': {'decomposition': ['When was Superhero invented?', 'When did digital format become available?', 'Is #1 before #2?']}, 'e45dd0b92d16cd22ea02': {'decomposition': ['How large are newborn American Black Bear cubs?', 'How large is a king size bed?', 'What is #2 divided by #1?', 'Is #3 less than or equal to #2?']}, '8efd6e6c6c104be8f9fe': {'decomposition': [\"What was Aldi's major food item?\", 'What is the reason behind #1 being out of date?', 'Was the reason behind #2 being so popular that it was never sold?']}, '896b0fa12e06b67a1b91': {'decomposition': ['What is the main ingredient of black pepper?', 'What is ground bell pepper?', 'Is #2 included in #1?']}, '0dc86ca8b9eb2c1f99dd': {'decomposition': ['What is the main purpose of taking ukemi?', 'What is the main purpose of #1?', 'Is kinetic energy included in #2?']}, 'ba2a5930ff574802a3da': {'decomposition': ['What kind of food is a BLT?', 'What kind of food is Casablanca known for?', 'Is #1 exclusive of #2?']}, '4d52dbeb76f228b14998': {'decomposition': ['How much snowdon is annually precipitated?', 'How much does an upright bowling pin weigh?', 'Is #2 greater than #1?']}, '1eb877fa5b2962f5e5a9': {'decomposition': ['When was Stanley Baldwin born?', 'When was the woman Prime Minister of the United Kingdom?', 'Is #2 before #1?']}, '6208234549bac8cc46da': {'decomposition': [\"What is the bald eagle's song in B-52?\", 'What is the song containing #1?', \"Which song is the song 'Cry Me A River' by Dre?\", 'Is #2 an urgent message before #3?']}, 'a9d22dce3199e77dd271': {'decomposition': ['What is the average price for a professional boxer?', 'What is the average price for a low dental bill?', 'Is #1 the same as #2?']}, '14bfe3d51a876549fc0a': {'decomposition': ['How did Dr. Seuss die?', 'Did #1 not involve a tragedy?']}, '157b095bcd5d48c694f5': {'decomposition': [\"Who was Jackson Pollock's father?\", 'What religion was #1 not associated with?', 'Which religion was The Pledge of Allegiance as a child?', 'Is #2 different from #3?']}, '5787a201b7da5929cb75': {'decomposition': ['Which city did the San Diego County home to?', 'Which religious group was #1 home to?', 'Is #2 the same as Shamu?']}, '81f2c649b9f61e019fcf': {'decomposition': ['What was the flying Spaghetti Monster?', 'What was the name of the ancient pantheon that was once located in #1?', 'Is #2 the same as #2?']}, '1d07c9eb265c6cdc3947': {'decomposition': ['Which period is the Balearic Islands in?', 'When did Elizabeth II reign?', 'Is #2 before #1?']}, '8c115e85cf8c30135bf4': {'decomposition': ['How long did the Football War last?', 'How many days are in a month?', 'Is #1 longer than #2?']}, '2b98270f27a5263b991f': {'decomposition': ['When did Lorenzo de Medici become the patronage of?', 'When was Da Vinci the patronage of?', 'Is #2 before #1?']}, '9f141949c05a47064fa9': {'decomposition': ['Islam celebrates the last Supper?', 'Which foods are forbidden in #1?', 'Is any of #2 included in Islam?']}, 'b37207ee5c3ebd455355': {'decomposition': ['What is the range of the Bengal cat?', 'What are the range of the fish caught in #1?', 'Is fish part of #2?']}, '71e6d5a927e7241c8f59': {'decomposition': ['Which shoes were worn by Benito Mussolini during the Hafór Björnsson during the match?', 'How many shoes were worn by Benito Mussolini during the match?', 'Is #2 larger than #1?']}, 'bdaf032b5e375aeb9bfa': {'decomposition': [\"What is Bobby Jindal's religion?\", 'In what religion is kibble celebrated?', 'Is #1 the same as #2?']}, 'c39d1e6ffbca58600a38': {'decomposition': ['Where is the White House located?', 'Where is the Capitol located?', 'Is #2 located near #1?']}, '5b45ccb915731a9e5f05': {'decomposition': ['What is the number of loggers that can be found in the US?', 'Is #1 less than 100?']}, 'c44aebb62354b76e5f68': {'decomposition': ['Who was the father of Iggy Pop?', \"Who was #1's father?\", 'Was #2 the same as his father?']}, '3eb805c6e6cce883416c': {'decomposition': ['What is the spread of STI?', 'What processes are involved in preventive healthcare?', 'Does any of #1 have a lower chance of transmission of #2?']}, 'b81db0d01318a28e877c': {'decomposition': ['Which of the major items are inanimate objects?', 'Is the Beauty and the Beast included in #1?']}, 'ed247b0929da4a53aa85': {'decomposition': ['What music-related occupation does the band Bandy have?', 'What kinds of events are held in Texas?', 'Is any of #1 held in #2?']}, '9bf48fb011c640f0de4b': {'decomposition': ['What nutrients are important for bone growth?', 'What are the ingredients in spinach that are consumed?', 'Is any of #1 included in #2?']}, '5a7265daf4314e5b9283': {'decomposition': ['What is the average temperature of Amazonas?', 'What is the risk of someone to die from?', 'Is #1 or #2 higher than #3?']}, '31d0f85e4da977a2ada5': {'decomposition': ['Which console did Nintendo Playstation 3 come with?', 'What is the original Nintendo game?', 'What format did #2 play for consoles after #1?', 'Is #3 the same as Playstation 3?']}, '0ec75be0266c3bc29a85': {'decomposition': ['How many numbers are found in the Fibonacci sequence?', 'How many numbers are discovered in Pi?', 'Is #1 greater than #2?']}, 'ec0463f1bd3c28fe300a': {'decomposition': ['What is referred to as Judo?', 'What activities are involved in #1?', 'Do Germaphobia involve participating in #2?']}, '72478c2c5ce262b5cef4': {'decomposition': ['What are the seasons in the US?', 'When do bear pelts occur?', 'Is autumn included in #1?']}, '7e6f36cc5ad38b425b53': {'decomposition': ['What are the triggers for eating disorder?', 'Does any of #1 trigger any of the following music?']}, '5f290c4202a54bf6aa71': {'decomposition': [\"What was Al Unser Jr's top speed?\", 'What is the top speed of the Space Race?', 'Is #2 greater than #1?']}, '534cebe828195c3a8f31': {'decomposition': ['How much does the Warcraft story weigh?', 'What is the weight of a loaf of bread?', 'Is #1 greater than #2?']}, 'f640e31952dea1040e3c': {'decomposition': ['When did Electronic Arts acquire?', 'What was the budget of Metroid?', 'Is #2 greater than #1?']}, '2800bf5c809ed1224b42': {'decomposition': ['What is the New Testament of the Bible?', 'What is a kindergarten teacher?', 'Is #1 the same as #2?']}, '6e95d89ccd3256bde343': {'decomposition': ['What must you always perform to be live?', \"What is Amy Winehouse's profession?\", 'Are #1 and #2 the same?']}, '7d1f1c2a9d554a017a22': {'decomposition': ['What are the ingredients of anchovy pizza?', 'Do bones are found in #1?']}, '4070e4485b1bcc9e04d8': {'decomposition': ['How big is a basketball cat?', \"How big is a sand cat's ear?\", 'Is #2 smaller than #1?']}, '6ef94676fb10c0e34de2': {'decomposition': [\"Who was Ivan the Terrible's father?\", 'How many nicknames did #1 have?', 'How many nicknames did Ivan the Terrible have?', 'Is #2 greater than #3?']}, '42852587e89ab6ade6b9': {'decomposition': ['What are the causes of breast cancer?', 'What are the causes of breast cancer?', 'Are amoebaeba included in #2?']}, 'bac108d947b7b26035cc': {'decomposition': ['How many children were in the 1980s?', 'What was the minimum number of children that was in the 1980s?', 'Is #1 greater than #2?']}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Loading decomposition data for validation\n",
        "\n",
        "Here, we load the decomposition data for validation and initialize relevant data structures for testing the best values of doc_num and k for optimal document retrieval."
      ],
      "metadata": {
        "id": "uJlHo_-wpF79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('final_output_test_30_5.json', 'w') as f:\n",
        "  json.dump(final_output, f, indent=4)"
      ],
      "metadata": {
        "id": "tEAv5hvyFwzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# doc nums and k-values to experiment with\n",
        "doc_nums = [10, 15, 20, 25, 30]\n",
        "k_values = [2, 5, 8, 10]"
      ],
      "metadata": {
        "id": "AmlGJvEdMGiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Validation\n",
        "\n",
        "Here we actually test the model on our decompositions from experiment part 1 to see what the maximum accuracy rate we are able to achieve is."
      ],
      "metadata": {
        "id": "QLM8zHp1pgWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for num in doc_nums:\n",
        "  for k_val in k_values:\n",
        "    print(f\"Currently testing {num}, {k_val}\")\n",
        "    final_output = {}\n",
        "    print(f\"num decomps: {len(decomposition_data.items())}\")\n",
        "    for qid, decomps in decomposition_data.items():\n",
        "      decomps = list(decomps.values())[0]\n",
        "      answers, docs = processQuestion(decomps, num, k_val)\n",
        "      final_output[qid] = {'answer': True if answers[-1] == \"true\" else False, 'decomposition': decomps, 'paragraphs': docs}\n",
        "\n",
        "    file_path = f'final_output_test_{num}_{k_val}.json'\n",
        "    with open(file_path, 'w') as f:\n",
        "      json.dump(final_output, f, indent=4)"
      ],
      "metadata": {
        "id": "6UmA17hVMS5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: delete runtime before running\n",
        "!python ./strategyqa_v2/src/evaluators/evaluate_all.py --golds_file ./strategyqa_v2/data/dev.json --predictions_file \"final_output_test_modified_bloom560m_20_5.json\""
      ],
      "metadata": {
        "id": "r3zSF1MLrqCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Fine-tuning\n",
        "\n",
        "This section contains our code for fine-tuning our intermediate QA model on the DROP (Discrete Reasoning Over the content of Paragraphs) dataset. By training on this dataset, we mainly aimed to improve the ability of our model to search through provided information to obtain the answers to questions that are harder to answer due to their implicit nature. With the DROP dataset, the model is essentially provided with a paragraph and asked to answer a related question which is answered in the paragraph. Since this is similar to the StrategyQA task, where the Wikipedia corpus contains information, we hoped that finetuning our model by training on this dataset would help improve our accuracy.\n",
        "\n",
        "However, we generally found that our accuracy actually decreased. We anticipate this may be due to the fact-retrieval and multiple-stage QA aspects of the StrategyQA task that make it more complex than training on the DROP dataset."
      ],
      "metadata": {
        "id": "EYHqSN7olcoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "data_train = json.load(open(\"/content/strategyqa_v2/data/drop_dataset_train.json\"))\n",
        "data_dev = json.load(open(\"/content/strategyqa_v2/data/drop_dataset_dev.json\"))\n",
        "train_dataset = []\n",
        "val_dataset = []\n",
        "\n",
        "def formatDropData(data, formatted_data):\n",
        "  for d in data.keys():\n",
        "    context = data[d]['passage']\n",
        "\n",
        "    for info in data[d]['qa_pairs']:\n",
        "      question = info['question']\n",
        "      ans = info['answer']\n",
        "      answer = \"\"\n",
        "\n",
        "      answer += ans['number']\n",
        "\n",
        "      if ('day' in ans['date'].keys() and ans['date']['day'] != \"\"):\n",
        "        answer += ans['date']['day'] + \" \" + ans['date']['month'] + \" \" + ans['date']['year']\n",
        "\n",
        "      answer += \" \".join(ans['spans'])\n",
        "\n",
        "      formatted_data.append({\"question\": question, \"context\": context, \"answer\": answer})\n",
        "\n",
        "formatDropData(data_train, train_dataset)\n",
        "formatDropData(data_dev, val_dataset)"
      ],
      "metadata": {
        "id": "j1TkJ2ZNRIZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, TensorDataset\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Initialize the tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained('mariOrOsSi/t5-base-finetuned-question-answering')\n",
        "model = T5ForConditionalGeneration.from_pretrained('mariOrOsSi/t5-base-finetuned-question-answering').to(DEVICE)\n",
        "\n",
        "# Function to prepare data for T5\n",
        "def prepare_data_for_t5(questions, contexts, answers):\n",
        "    input_texts = []\n",
        "    target_texts = []\n",
        "    for question, context, answer in zip(questions, contexts, answers):\n",
        "        input_text = f\"question: {question} context: {context}\"\n",
        "        target_text = answer\n",
        "        input_texts.append(input_text)\n",
        "        target_texts.append(target_text)\n",
        "    return input_texts, target_texts\n",
        "\n",
        "# Prepare data\n",
        "train_questions = [d['question'] for d in train_dataset]\n",
        "train_contexts = [d['context'] for d in train_dataset]\n",
        "train_answers = [d['answer'] for d in train_dataset]\n",
        "\n",
        "train_input_texts, train_target_texts = prepare_data_for_t5(train_questions, train_contexts, train_answers)\n",
        "\n",
        "# Tokenize data\n",
        "def tokenize_for_t5(input_texts, target_texts, tokenizer):\n",
        "    model_inputs = tokenizer(input_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(target_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=128).input_ids\n",
        "    return TensorDataset(model_inputs.input_ids, model_inputs.attention_mask, labels)\n",
        "\n",
        "train_dataset = tokenize_for_t5(train_input_texts, train_target_texts, tokenizer)\n",
        "train_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=8)\n",
        "\n",
        "# Training loop\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(1):  # ideally for 3 epochs\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
        "        batch = tuple(t.to(DEVICE) for t in batch)\n",
        "        inputs = {\n",
        "            \"input_ids\": batch[0],\n",
        "            \"attention_mask\": batch[1],\n",
        "            \"labels\": batch[2],\n",
        "        }\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")\n"
      ],
      "metadata": {
        "id": "Fc6Du44vOwNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = \"/content/drive/MyDrive/finetuned-models/finetuned-t5-base-qa\"\n",
        "\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "model.save_pretrained(model_save_path)\n"
      ],
      "metadata": {
        "id": "axgHP73PThXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from  transformers  import  AutoTokenizer, AutoModelWithLMHead, pipeline\n",
        "\n",
        "general_qa_tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/finetuned-models/finetuned-t5-base-qa\")\n",
        "general_qa_model = AutoModelWithLMHead.from_pretrained(\"/content/drive/MyDrive/finetuned-models/finetuned-t5-base-qa\").to(DEVICE)"
      ],
      "metadata": {
        "id": "98WRc6GoUu1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **THE SECTION BELOW IS UNUSED CODE**"
      ],
      "metadata": {
        "id": "_Znc1acqApg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jsonlines\n",
        "decomposition_data = []\n",
        "\n",
        "with jsonlines.open('./strategyqa_v2/data/generated/t5predictions.jsonl') as reader:\n",
        "    for obj in reader:\n",
        "        decomposition_data.append(obj)"
      ],
      "metadata": {
        "id": "JB2IvXe0AsfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_output = {}\n",
        "for data in decomposition_data:\n",
        "  print(data)\n",
        "  answers, docs = processQuestion(data)\n",
        "  final_output[data['qid']] = {'answer': True if answers[-1] == \"true\" else False, 'decomposition': data['predicted_decomposition'], 'paragraphs': docs}\n",
        "\n",
        "with open('./drive/MyDrive/UW/CSE 447/Final Project/NLP/final_output.json', 'w') as f:\n",
        "  json.dump(final_output, f, indent=4)"
      ],
      "metadata": {
        "id": "jPbyG8LBAxbc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}