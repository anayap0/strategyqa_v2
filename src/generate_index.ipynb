{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get wikipedia corpus\n",
    "!wget https://storage.googleapis.com/ai2i/strategyqa/data/corpus-enwiki-20200511-cirrussearch-parasv2.jsonl.gz\n",
    "!gzip -dv corpus-enwiki-20200511-cirrussearch-parasv2.jsonl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format corpus so that index can be created\n",
    "# this is directly copied from https://github.com/neuralmind-ai/visconde/blob/main/strategyqa_create_indices.py\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "stride = 4\n",
    "max_length = 5\n",
    "\n",
    "def window(document, stride=stride, max_length=max_length):\n",
    "    doc = nlp(document[:10000])\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "    if len(sentences) > max_length:\n",
    "        new_documents = []\n",
    "        for i in range(0, len(sentences), stride):\n",
    "            segment = ' '.join(sentences[i:i + max_length])\n",
    "            new_documents.append(segment)\n",
    "        return new_documents\n",
    "    return [document]\n",
    "\n",
    "\n",
    "print(\"Creating strategyQA pyserin indices\")\n",
    "\n",
    "f = open(\"data/corpus-enwiki-20200511-cirrussearch-parasv2.jsonl\",'r')\n",
    "file_limit = 1000000\n",
    "file_items_count = 0\n",
    "file_number = 0\n",
    "\n",
    "nf = open(\"strategyqa_pyserin_files/{0}.jsonl\".format(file_number),\"w\")\n",
    "id_ = 1\n",
    "for l in tqdm(f):\n",
    "    item = json.loads(l)\n",
    "    docs = window(item['para'])\n",
    "    # docs = [item['para']]\n",
    "    for d in docs:\n",
    "        item['contents'] = \"Title: {0}. Content: {1}\".format(item[\"title\"],d)\n",
    "        item['m_id'] = item[\"title\"]+\"-\"+str(item['para_id'])\n",
    "        item['id'] = id_\n",
    "        id_ += 1\n",
    "        if \"para\" in item:\n",
    "            del item['para']\n",
    "        nf.write(json.dumps(item)+\"\\n\")\n",
    "        file_items_count = file_items_count + 1\n",
    "\n",
    "    if file_items_count >= file_limit:\n",
    "        file_number = file_number + 1\n",
    "        nf.close()\n",
    "        nf = open(\"strategyqa_pyserin_files/{0}.jsonl\".format(file_number),\"w\")\n",
    "        file_items_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "!python -m pyserini.index.lucene \\\n",
    "  --collection JsonCollection \\\n",
    "  --input drive/MyDrive/UW/\"CSE 447\"/\"Final Project\"/NLP/strategyqa_pyserin_files \\\n",
    "  --index indexes/sample_collection_jsonl \\\n",
    "  --generator DefaultLuceneDocumentGenerator \\\n",
    "  --threads 1 \\\n",
    "  --storePositions --storeDocvectors --storeRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample searches\n",
    "from pyserini.search import LuceneSearcher\n",
    "\n",
    "# search document\n",
    "searcher = LuceneSearcher('./drive/MyDrive/UW/CSE 447/Final Project/NLP/sample_collection_jsonl/')\n",
    "hits = searcher.search('What language is used in Saint Vincent and the Grenadines?')\n",
    "\n",
    "\n",
    "# print document contents\n",
    "print(len(hits))\n",
    "for hit in hits:\n",
    "  print(hit.score)\n",
    "  doc = searcher.doc(hit.docid)\n",
    "  print(doc.raw())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
